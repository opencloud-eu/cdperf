import{_ as e,o as a,c as t,O as s}from"./chunks/framework.9a1c14ea.js";const f=JSON.parse('{"title":"Performance Considerations","description":"","frontmatter":{},"headers":[],"relativePath":"k6-tests/docs/considerations.md","filePath":"packages/k6-tests/docs/considerations.md","lastUpdated":1751469544000}'),r={name:"k6-tests/docs/considerations.md"},i=s('<h1 id="performance-considerations" tabindex="-1">Performance Considerations <a class="header-anchor" href="#performance-considerations" aria-label="Permalink to &quot;Performance Considerations&quot;">​</a></h1><h2 id="how-can-we-model-typical-scenarios" tabindex="-1">How can we model typical scenarios? <a class="header-anchor" href="#how-can-we-model-typical-scenarios" aria-label="Permalink to &quot;How can we model typical scenarios?&quot;">​</a></h2><p>Getting comparable statement about performance of EFSS cloud installations is a non-trivial task.</p><p>To solve that, the idea is to model typical scenarios for EFSS cloud installations. That will make it possible to compare clouds objectively in terms of performance and stability.</p><p>There needs to be a standardized process to collect data that allows a valid comparison. It needs to be repeatable and clearly specified.</p><p>For that, we developed a tool called cdperf. cdperf is based on <a href="https://k6.io" target="_blank" rel="noreferrer">k6</a>. It automates running certain test setups for currently three different test candidates: ownCloud 10, OpenCloud and NextCloud.</p><h2 id="influential-parameters" tabindex="-1">Influential Parameters <a class="header-anchor" href="#influential-parameters" aria-label="Permalink to &quot;Influential Parameters&quot;">​</a></h2><p>First, it makes sense to think about all parameters that influence the performance of an EFSS system.</p><h3 id="hardware" tabindex="-1">Hardware <a class="header-anchor" href="#hardware" aria-label="Permalink to &quot;Hardware&quot;">​</a></h3><p>It is important to use capable hardware for the test. Capable means that enough resources on the computer are available for the test, so that during testing no hardware limits are reached. That is especially important if networking is involved.</p><p>Tests that should be compared need to be run on the same hardware in the same environment and on the same operating system.</p><h3 id="machine-setup" tabindex="-1">Machine Setup <a class="header-anchor" href="#machine-setup" aria-label="Permalink to &quot;Machine Setup&quot;">​</a></h3><p>Leaving things like hardware and operating system aside, the setup, ie. a virtualization- or bare metal setup, is important. To make it easy to repeat tests with different clouds on the same system, cdperf is using docker to run the cloud installations and the test tool.</p><p>By default, it uses the officially provided docker containers of the cloud flavours. The installations within might be a more or less optimal setup.</p><h3 id="runtime-configuration" tabindex="-1">Runtime Configuration <a class="header-anchor" href="#runtime-configuration" aria-label="Permalink to &quot;Runtime Configuration&quot;">​</a></h3><p>Especially for a distributed system like OpenCloud it is important how the runtime manages the services. Are they run as separate processes, system threads or go co-routines? That influences how many operations can be parallelized and how many processor cores can be used at the same time.</p><h3 id="concurrent-access" tabindex="-1">Concurrent Access <a class="header-anchor" href="#concurrent-access" aria-label="Permalink to &quot;Concurrent Access&quot;">​</a></h3><p>To model a real life scenario of EFSS it is important to mimic parallel access to the system. That means that many virtual users access the cloud independently at the same time.</p><p>In addition to that it happens that for example the desktop client runs parallel requests to the server to achieve one job, i.e. a file upload. That triggers PUT requests that run in parallel.</p><h3 id="file-structure" tabindex="-1">File Structure <a class="header-anchor" href="#file-structure" aria-label="Permalink to &quot;File Structure&quot;">​</a></h3><p>Enterprise file sync and share are about unstructured data or just file. The performance of the EFSS system is influenced by</p><ul><li>the number of files</li><li>their size</li><li>the histogram of the sizes of a set of files</li><li>the number of directories</li><li>the depth of the file tree</li></ul><p>cdperf uses a histogram of file sizes that was seen on our instances with many users as average.</p><h3 id="sharing" tabindex="-1">Sharing <a class="header-anchor" href="#sharing" aria-label="Permalink to &quot;Sharing&quot;">​</a></h3><p>The number of shares on the files that are involved in performance metering influences the overall performance, as well as the size of the single involved shares.</p><p>Examples:</p><ol><li>One file shared with 1000 people.</li><li>A directory with depth 30 and 10,000 files shared with one person.</li><li>1000 different file- and/or directory shares in a user&#39;s file base.</li></ol><h3 id="request-baseline" tabindex="-1">Request Baseline <a class="header-anchor" href="#request-baseline" aria-label="Permalink to &quot;Request Baseline&quot;">​</a></h3><p>Depending on the setup of the whole EFSS system with clients, there is a ground noise of requests that happen without any user interaction, mainly through clients that check for availability of the cloud and sync states. This must be considered in performance checks.</p>',29),o=[i];function n(l,h,c,d,p,u){return a(),t("div",null,o)}const b=e(r,[["render",n]]);export{f as __pageData,b as default};
